{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name:\n",
    "#### Student ID:\n",
    "\n",
    "Date: 02/10/2022\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* nltk\n",
    "* itertools\n",
    "* sklearn\n",
    "* numpy\n",
    "* re\n",
    "* os\n",
    "\n",
    "## Introduction\n",
    "In this task, I will conduct basic text pre-processing on the given dataset, such as tokenization and deleting most/least frequent words and stop words. We are just to concentrate on pre-processing the job description for this task. After the data has been preprocessed, I will create a vocabulary using the cleaned job advertisement descriptions, save it in a txt file `vocab.txt`, and then save the text and information for all job advertisements in txt files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import *\n",
    "from itertools import chain\n",
    "from sklearn.datasets import load_files\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders in the data folder: 4\n",
      "Job categories in the data folder: Accounting_Finance, Engineering, Healthcare_Nursing, Sales\n",
      "Number of job advertisments (documents): 776\n",
      "\n",
      "Format of the txt files:\n",
      "\n",
      "Title: Finance / Accounts Asst Bromley to ****k\n",
      "Webindex: 68997528\n",
      "Company: First Recruitment Services\n",
      "Description: Accountant (partqualified) to **** p.a. South East London Our client, a successful manufacturing company has an immediate requirement for an Accountant for permanent role in their modern offices in South East London. The Role: Credit Control Purchase / Sales Ledger Daily collection of debts by phone, letter and email. Handling of ledger accounts Handling disputed accounts and negotiating payment terms Allocating of cash and reconciliation of accounts Adhoc administration duties within the business The Person The ideal candidate will have previous experience in a Credit Control capacity, you will possess exceptional customer service and communication skills together with IT proficiency. You will need to be a part or fully qualified Accountant to be considered for this role\n"
     ]
    }
   ],
   "source": [
    "# Code to inspect the provided data file...\n",
    "job_data = load_files(r\"data\") # reading the files from the data folder\n",
    "print('Number of folders in the data folder:',len(job_data.target_names),) # number of folders(job categories)\n",
    "print('Job categories in the data folder:', end=' '), print(*job_data.target_names, sep=', ') # name of the job categories\n",
    "print('Number of job advertisments (documents):', len(job_data.data))\n",
    "print('\\nFormat of the txt files:\\n') # example of the structure of a job advertisment\n",
    "print(job_data.data[0].decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above output, there are four different subfolders within the data folder, namely  Accounting_Finance, Engineering, Healthcare_Nursing, and Sales, with each folder name representing a job category. A certain category's text documents for job advertisements can be found in the associated subfolder. Each job advertisement is a `.txt` file with the name `Job_<ID>.txt.`. It includes the title, the Webindex, and the entire job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of job descriptions in the list: 776\n",
      "\n",
      "First job advertisment in job_data:\n",
      "\n",
      "Title: Finance / Accounts Asst Bromley to ****k\n",
      " Webindex: 68997528\n",
      " Description: Accountant (partqualified) to **** p.a. South East London Our client, a successful manufacturing company has an immediate requirement for an Accountant for permanent role in their modern offices in South East London. The Role: Credit Control Purchase / Sales Ledger Daily collection of debts by phone, letter and email. Handling of ledger accounts Handling disputed accounts and negotiating payment terms Allocating of cash and reconciliation of accounts Adhoc administration duties within the business The Person The ideal candidate will have previous experience in a Credit Control capacity, you will possess exceptional customer service and communication skills together with IT proficiency. You will need to be a part or fully qualified Accountant to be considered for this role\n"
     ]
    }
   ],
   "source": [
    "# Extracting Titles, Webindices, and Descriptions into separate lists\n",
    "file_names = []\n",
    "titles = []\n",
    "webindices = []\n",
    "descriptions = []\n",
    "\n",
    "for fname in job_data.filenames:\n",
    "    file_names.append(fname)\n",
    "\n",
    "for j in job_data.data:\n",
    "    titles.append(re.findall(r\"(?<=Title: ).*\\n\", j.decode())) # appending the title of each job using regex to [titles]\n",
    "    webindices.append(re.findall(r\"(?<=Webindex: )\\d+\\n\", j.decode())) # using decode() to tranform to a string fotmat\n",
    "    descriptions.append(re.findall(r\"(?<=Description: ).+\", j.decode())) # Search for the description of each job using regex\n",
    "print('Number of job descriptions in the list:', len(descriptions))\n",
    "print('\\nFirst job advertisment in job_data:\\n')\n",
    "print('Title:',*titles[0],'Webindex:',*webindices[0],'Description:',*descriptions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code block, I created a function for tokenization, lowercasing, removing words of `length < 2` and removing stop words. The function takes raw string as an argument and returns a list of tokenized strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for tokenizaton, lowercase conversion, removing words less than 2, removing stop words ...\n",
    "def tokenizeRawData(desc):\n",
    "    \n",
    "    stopwords = []\n",
    "    with open('stopwords_en.txt') as f: # Read the sopwords list\n",
    "        stopwords = f.read().splitlines()\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\") # Word Tokenizer structer in regex\n",
    "    sentences = sent_tokenize(desc) # Tokenize job descriptions into sentences\n",
    "    token_lists = [tokenizer.tokenize(sen) for sen in sentences] # Tokenize each sentence in the job description\n",
    "    tokens = list(chain.from_iterable(token_lists)) # Flatten the lists of words (from sentences) into a single list of words\n",
    "    tokenized = [token.lower() for token in tokens] # Convert into lower case\n",
    "    len2words = [token for token in tokenized if len(token) >= 2] # Removing words with length less than 2\n",
    "    stopped = [token for token in len2words if token not in stopwords] # Removing stop words\n",
    "    \n",
    "    return stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_desc = [tokenizeRawData(desc[0]) for desc in descriptions] # Tokenize each job description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, I implemented the same function for statistics from the pre-class activites. The function will print several statistics including:\n",
    "* The total number of tokens across the corpus\n",
    "* The total size of vocabulary (types)\n",
    "* The lexical diversity which refers to the ratio of different unique word stems (types) to the total number of words (tokens). \n",
    "* The average, minimum and maximum number of tokens in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_print(tokenised_desc):\n",
    "    words = list(chain.from_iterable(tokenised_desc)) # we put all the tokens in the corpus in a single list\n",
    "    vocab = set(words) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of job decriptions:\", len(tokenised_desc))\n",
    "    lens = [len(desc) for desc in tokenised_desc]\n",
    "    print(\"Average job decription length:\", np.mean(lens))\n",
    "    print(\"Maximum job decription length:\", np.max(lens))\n",
    "    print(\"Minimum job decription length:\", np.min(lens))\n",
    "    print(\"Standard deviation of job decription length:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9404\n",
      "Total number of tokens:  107161\n",
      "Lexical diversity:  0.0877558066834016\n",
      "Total number of job decriptions: 776\n",
      "Average job decription length: 138.09407216494844\n",
      "Maximum job decription length: 487\n",
      "Minimum job decription length: 12\n",
      "Standard deviation of job decription length: 73.07847897002313\n"
     ]
    }
   ],
   "source": [
    "stats_print(tokenised_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency measures how frequently a word appears across the entire corpus, regardless of the document it appears in. We can determine the distribution of the total number of word tokens among all the types using the frequency distribution based on term frequency. We find the frequency of words by using `FreqDist` function from `nltk` package, we can then use `hapaxes` function to find the words that only appear once in the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that appear only once in the document collection based on term frequency is: 4186\n"
     ]
    }
   ],
   "source": [
    "# Removing words that appear once based on term frequency\n",
    "words = list(chain.from_iterable(tokenised_desc))\n",
    "term_freq = FreqDist(words) # compute term frequency for each unique word\n",
    "lessFreqWords = set(term_freq.hapaxes())\n",
    "print('Number of words that appear only once in the document collection based on term frequency is:', len(lessFreqWords))\n",
    "for i in range(len(tokenised_desc)): # for loop to iterate through the dictionary items\n",
    "    removed = [w for w in tokenised_desc[i] if w not in lessFreqWords]\n",
    "    tokenised_desc[i] = removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5218\n",
      "Total number of tokens:  102975\n",
      "Lexical diversity:  0.05067249332362224\n",
      "Total number of job decriptions: 776\n",
      "Average job decription length: 132.69974226804123\n",
      "Maximum job decription length: 471\n",
      "Minimum job decription length: 12\n",
      "Standard deviation of job decription length: 70.3782402519735\n"
     ]
    }
   ],
   "source": [
    "stats_print(tokenised_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of documents in which a word appears in is referred to as document frequency. For instance, if a word appears three times in a document, the number of occurrences for the term will count as one and it will increase by `1` if it appears in another document. To find the top 50 most frequent words based on document frequency we use `most_common()` function which returns the frequency of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 50 most frequent words based on document frequency are: experience, role, work, team, working, skills, client, job, company, business, uk, excellent, management, based, apply, opportunity, salary, required, successful, support, join, candidate, service, knowledge, development, leading, high, cv, www, manager, training, sales, strong, provide, including, services, ability, contact, position, recruitment, full, benefits, posted, jobseeking, originally, clients, include, good, essential, information\n"
     ]
    }
   ],
   "source": [
    "# Removing top 50 most frequent words based on document frequency\n",
    "words_50 = list(chain.from_iterable([set(tok_desc) for tok_desc in tokenised_desc]))\n",
    "doc_freq = FreqDist(words_50)\n",
    "top_50 = doc_freq.most_common(50)\n",
    "top_50 = [w[0] for w in top_50]\n",
    "print('The top 50 most frequent words based on document frequency are:', ', '.join(top_50))\n",
    "for i in range(len(tokenised_desc)): # for loop to iterate through the dictionary items\n",
    "    remove_top50 = [w for w in tokenised_desc[i] if w not in top_50]\n",
    "    tokenised_desc[i] = remove_top50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  5168\n",
      "Total number of tokens:  81205\n",
      "Lexical diversity:  0.06364140139153993\n",
      "Total number of job decriptions: 776\n",
      "Average job decription length: 104.64561855670104\n",
      "Maximum job decription length: 401\n",
      "Minimum job decription length: 7\n",
      "Standard deviation of job decription length: 58.44628718710534\n"
     ]
    }
   ],
   "source": [
    "stats_print(tokenised_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "- vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this task contains the file `vocab.txt`. This file contains the unigram vocabulary, one each line, in the format of `word_string:word_integer_index` sorted in alphabetical order. Also, in the last code block the for loop creats a new data folder named `preprocessed_data` with all the corresponding job categories and cleand job advertisements saved to its particular category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save output data...\n",
    "words = list(chain.from_iterable(tokenised_desc)) # Combine all the tokens into one list\n",
    "vocab = sorted(set(words)) # Get the unique vocab sorted alphabetically\n",
    "\n",
    "with open('vocab.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    for i in range(len(vocab)): # loob through the len of vocab list\n",
    "        file.write(vocab[i] + \":\" + str(i) + '\\n') # write each vocab and the corresponding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# saving the preprocessed job ads\n",
    "for i in range(len(file_names)):\n",
    "    filename = (\"preprocessed_\" + file_names[i]) #setting the directory and the file name for each job ad\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True) # create a directory if it doesn't exist\n",
    "    with open(filename, 'w', encoding=\"utf-8\") as file:\n",
    "        file.write(\"Title: \" + str(*titles[i])) # write the job title\n",
    "        file.write(\"Webindex: \" + str(*webindices[i])) # write the webindex for the job\n",
    "        desc = ' '.join(tokenised_desc[i]) # concatenate each token of the job description into one string\n",
    "        file.write(\"Description: \" + desc) # write the preprocessed job description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "To summarise what I did in this task, I first performed basic text pre-processing on the provided dataset, such as tokenization and deleting most/least common words and stop words while focusing just on the job description. Using the cleaned job advertisement descriptions as a starting point, I built a vocabulary and placed it in a txt file named \"vocab.txt,\" after which I recorded the text and details for all of the job advertisements in txt files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
