{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone I Natural Language Processing\n",
    "## Task 2 & 3\n",
    "#### Osama Alfawzan\n",
    "\n",
    "Date: 02/10/2022\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* sklearn\n",
    "* gensim.downloader\n",
    "* logging\n",
    "\n",
    "## Introduction\n",
    "For the collection of job advertisements, I will create various feature representations as part of this task. We will only take into account the job description in this task. The feature representations I'll create are as follow: The count vector representation for each job advertisement description is contained in the bag-of-words model, which I then save it to a file called count_vectors.txt. I will then generate the TF-IDF weighted and unweighted vector representations for each job advertisement description using the GoogleNews300 embedding language model. After that, in Task 3 I will build machine learning models for classifying the category of a job advertisement text. For this particular task I will use the logistic regression model from sklearn. Finally, I will conduct two sets of experiments on the provided dataset to compare the three different language models, then investigate if more information could lead to a higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_files\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to observe the progress of downloading GoogleNews300 model\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Generating Feature Representations for Job Advertisement Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code block, I read the output files from Task 1 for the preprocessed job advertisments into a container which includes all the folders, subfolders, and .`txt` files. In the data folder, we have 4 different job categories which are `Accounting_Finance`, `Engineering`, `Healthcare_Nursing`, and `Sales` that correspong to the labels ordered as [0, 1, 2, 3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folders in the preprocessed data folder: 4\n",
      "Job categories in the preprocessed data folder: Accounting_Finance, Engineering, Healthcare_Nursing, Sales\n",
      "Number of job advertisments: 776\n"
     ]
    }
   ],
   "source": [
    "# Reading the job advertisements .txt files\n",
    "job_data = load_files(r\"preprocessed_data\") # reading the files from the preprocessed data folder\n",
    "print('Number of folders in the preprocessed data folder:',len(job_data.target_names)) # number of folders(job categories)\n",
    "print('Job categories in the preprocessed data folder:', end=' '), print(*job_data.target_names, sep=', ') # name of the job categories\n",
    "print('Number of job advertisments:', len(job_data.data))\n",
    "\n",
    "# Reading the vocab.txt file to use it as a base for CountVectorizer\n",
    "with open('vocab.txt') as f:\n",
    "    vocab_raw = [l.split(':') for l in f.read().splitlines()]\n",
    "vocab = [w[0] for w in vocab_raw] # using list comprehension to remove the index of vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, the code puts the job advertisments features into separate lists which in my opinion, is the most suitable data type for this task. The index of a specific list corresponds to the infromation about the job in the other lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of job descriptions in the list: 776\n",
      "\n",
      "First job advertisment in job_data:\n",
      "\n",
      "Title: Finance / Accounts Asst Bromley to ****k \n",
      "Webindex: 68997528 \n",
      "Description: accountant partqualified south east london manufacturing requirement accountant permanent modern offices south east london credit control purchase ledger daily collection debts phone letter email handling ledger accounts handling accounts negotiating payment terms cash reconciliation accounts adhoc administration duties person ideal previous credit control capacity possess exceptional customer communication part fully qualified accountant considered\n"
     ]
    }
   ],
   "source": [
    "# Extracting Titles, Webindices, and Descriptions into separate lists\n",
    "titles = []\n",
    "webindices = []\n",
    "descriptions = []\n",
    "categories = []\n",
    "\n",
    "for i in job_data.target:\n",
    "    categories.append(i)\n",
    "\n",
    "for j in job_data.data:\n",
    "    titles.append(re.findall(r\"(?<=Title: ).*(?=\\r\\n)\", j.decode())) # appending the title of each job using regex to [titles]\n",
    "    webindices.append(re.findall(r\"(?<=Webindex: )\\d+\", j.decode())) # using decode() to tranform to a string fotmat\n",
    "    descriptions.append(re.findall(r\"(?<=Description: ).+\", j.decode())) # Search for the description of each job using regex\n",
    "print('Number of job descriptions in the list:', len(descriptions))\n",
    "print('\\nFirst job advertisment in job_data:\\n')\n",
    "print('Title:',*titles[0],'\\nWebindex:',*webindices[0],'\\nDescription:',*descriptions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vector is represented as `word:count`, indicating how many times a word appears in a document. The following code generates the count vector for the job descriptions using `CountVectorizer` class from `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(776, 5168)\n"
     ]
    }
   ],
   "source": [
    "flat_descriptions = [d for desc in descriptions for d in desc] # flatten the list of lists of descriptions\n",
    "cVectorizer = CountVectorizer(analyzer = \"word\",vocabulary = vocab) # initialising the CountVectorizer\n",
    "count_features = cVectorizer.fit_transform(flat_descriptions) # generate the count vector representation for all articles\n",
    "print(count_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained word2vec model based on  GoogleNews300:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier handling of the data through all the coming tasks, I've decided to create a pandas data frame for the job advertisments with each required job feature as column in the data frame. The columns are: `Webindex`, `Title`, `Description`, `tk_Descriptions`, and `Category`. `tk_Descriptions` is the tokens for each job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 15:19:49,650 : INFO : Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-10-02 15:19:49,650 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Webindex</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>tk_Descriptions</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68997528</td>\n",
       "      <td>Finance / Accounts Asst Bromley to ****k</td>\n",
       "      <td>accountant partqualified south east london man...</td>\n",
       "      <td>[accountant, partqualified, south, east, londo...</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68063513</td>\n",
       "      <td>Fund Accountant  Hedge Fund</td>\n",
       "      <td>hedge funds london recruiting fund accountant ...</td>\n",
       "      <td>[hedge, funds, london, recruiting, fund, accou...</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68700336</td>\n",
       "      <td>Deputy Home Manager</td>\n",
       "      <td>exciting arisen establish provider elderly car...</td>\n",
       "      <td>[exciting, arisen, establish, provider, elderl...</td>\n",
       "      <td>Healthcare_Nursing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67996688</td>\n",
       "      <td>Brokers Wanted Imediate Start</td>\n",
       "      <td>expanding recruiting junior trainee brokers ci...</td>\n",
       "      <td>[expanding, recruiting, junior, trainee, broke...</td>\n",
       "      <td>Accounting_Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71803987</td>\n",
       "      <td>RGN Nurses (Hospitals)  Penarth</td>\n",
       "      <td>rgn nurses hospitals fulltime part swiis hour ...</td>\n",
       "      <td>[rgn, nurses, hospitals, fulltime, part, swiis...</td>\n",
       "      <td>Healthcare_Nursing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Webindex                                     Title  \\\n",
       "0  68997528  Finance / Accounts Asst Bromley to ****k   \n",
       "1  68063513               Fund Accountant  Hedge Fund   \n",
       "2  68700336                       Deputy Home Manager   \n",
       "3  67996688             Brokers Wanted Imediate Start   \n",
       "4  71803987           RGN Nurses (Hospitals)  Penarth   \n",
       "\n",
       "                                         Description  \\\n",
       "0  accountant partqualified south east london man...   \n",
       "1  hedge funds london recruiting fund accountant ...   \n",
       "2  exciting arisen establish provider elderly car...   \n",
       "3  expanding recruiting junior trainee brokers ci...   \n",
       "4  rgn nurses hospitals fulltime part swiis hour ...   \n",
       "\n",
       "                                     tk_Descriptions            Category  \n",
       "0  [accountant, partqualified, south, east, londo...  Accounting_Finance  \n",
       "1  [hedge, funds, london, recruiting, fund, accou...  Accounting_Finance  \n",
       "2  [exciting, arisen, establish, provider, elderl...  Healthcare_Nursing  \n",
       "3  [expanding, recruiting, junior, trainee, broke...  Accounting_Finance  \n",
       "4  [rgn, nurses, hospitals, fulltime, part, swiis...  Healthcare_Nursing  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a data frame for the job advertisements for easier handling of the data\n",
    "titles = [t for title in titles for t in title] # flatten the list of titles\n",
    "webindices = [w[0] for w in webindices] # flatten the list of webindices\n",
    "tk_descriptions = [d.split(' ') for d in flat_descriptions] # create a list of description tokens\n",
    "# dictionaries for the column values\n",
    "df_dic = {'Webindex': webindices, 'Title':titles, 'Description':flat_descriptions,\n",
    "          'tk_Descriptions':tk_descriptions, 'Label':categories}\n",
    "jobs_df = pd.DataFrame(df_dic) # initiliase the df with our dictionary\n",
    "# create conditions to fill the column Category with the specific job category\n",
    "conditions = [jobs_df['Label'] == 0, jobs_df['Label'] == 1, jobs_df['Label'] == 2, jobs_df['Label'] == 3] \n",
    "choices = ['Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Sales']\n",
    "jobs_df['Category'] = np.select(conditions, choices, default=np.nan) # intiliase the category columns with the corresponding category\n",
    "jobs_df = jobs_df.drop('Label', axis=1) # dropping the labels column as it is redundant\n",
    "jobs_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF weighted vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain pre-trained embeddings from the web. We begin by downloading the pre-trained word2vec model from Google News."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 15:19:49,724 : INFO : loading projection weights from C:\\Users\\Osama/gensim-data\\word2vec-google-news-300\\word2vec-google-news-300.gz\n",
      "2022-10-02 15:20:27,666 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\\\Users\\\\Osama/gensim-data\\\\word2vec-google-news-300\\\\word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2022-10-02T15:20:27.666161', 'gensim': '4.2.0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "# Loading the pretrained model\n",
    "preTW2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 5168)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tVectorizer = TfidfVectorizer(analyzer = \"word\",vocabulary = vocab) # initialising the TfidfVectorizer\n",
    "tfidf_features = tVectorizer.fit_transform(flat_descriptions) # generate the tfidf vector representation for all descriptions\n",
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code stores the vector representation as format `word:weight` in a list of dictionaries called `tfidf_weights` to use it for generating weighted vectors. Each list item is a dictionary of vocabulary as keys and their TF-IDF weight as value for each job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a list of dictionaries for tfidf as word:weight\n",
    "num = tfidf_features.shape[0]\n",
    "tfidf_weights = [] # empty list of dictionaries for tfidf weights\n",
    "for i in range(0, num):\n",
    "    wordweight_dict = {} # initialize and empty dictionary for each decription\n",
    "    for word, value in zip(vocab, tfidf_features.toarray()[i]): \n",
    "        if value > 0:\n",
    "            wordweight_dict[word] = value # store the values (weights) in the dictionry keys (vocabs)\n",
    "    tfidf_weights.append(wordweight_dict) # append the dictionary to the weights list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `weighted_vectors` function takes the word embeddings dictionary, the tokenized text of descriptions, and the tfidf weights (list of word:weight dictionaries, one for each description) as arguments, and generates the job ads embeddings:\n",
    " 1. creates an empty dataframe `docs_vectors` to store the job ads embeddings of descriptions\n",
    "  2. it loop through every tokenized text:\n",
    "    - creates an empty dataframe `temp` to store all the word embeddings of the description\n",
    "    - for each word that exists in the word embeddings dictionary/keyedvectors, \n",
    "        - if the argument `tfidf` weights are empty `[]`, it sets the weight of the word as 1\n",
    "        - otherwise, retrieve the weight of the word from the corresponding word:weight dictionary of the article from  `tfidf`\n",
    "    - row bind the weighted word embedding to `temp`\n",
    "    - takes the sum of each column to create the document vector\n",
    "    - append the created document vector to the list of document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate weighted vector representation for job advertisements\n",
    "def weighted_vectors(wv,tk_txts,tfidf = []):\n",
    "    docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "\n",
    "    for i in range(0,len(tk_txts)):\n",
    "        tokens = list(set(tk_txts[i])) # get the list of distinct words of the discription\n",
    "\n",
    "        temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "        for w_ind in range(0, len(tokens)): # looping through each word of a single document and spliting through space\n",
    "            try:\n",
    "                word = tokens[w_ind]\n",
    "                word_vec = wv[word] # if word is present in embeddings\n",
    "                \n",
    "                if tfidf != []:\n",
    "                    word_weight = float(tfidf[i][word])\n",
    "                else:\n",
    "                    word_weight = 1\n",
    "                temp = temp.append(pd.Series(word_vec*word_weight), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "            except:\n",
    "                pass\n",
    "        doc_vector = temp.sum() # take the sum of each column(w0, w1, w2,........w300)\n",
    "        docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
    "    return docs_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_preTW2v = weighted_vectors(preTW2v,jobs_df['tk_Descriptions'],tfidf_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unweighted vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes:\n",
    "* wv, a word:embedding dictionary or KeyedVectors; and \n",
    "* tk_txts, a list of tokenized texts, each of a job description\n",
    "as argument, it then does the following to generate the list of embedding vector representations, each for a description:\n",
    "1. creates an empty dataframe `docs_vectors` to store the job ads embeddings of descriptions\n",
    "2. it loop through every tokenized text:\n",
    "    - creates an empty dataframe `temp` to store all the word embeddings of the article\n",
    "    - for each word that exists in the word embeddings dictionary/keyedvectors, row bind the word embedding to `temp`\n",
    "    - take the sum of each column to create the document vector\n",
    "    - append the created document vector to the list of document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unweighted_vectors(wv,tk_txts): # generate unweighted vector representation for job advertisements\n",
    "    docs_vectors = pd.DataFrame() # creating empty final dataframe\n",
    "    \n",
    "    for i in range(0,len(tk_txts)):\n",
    "        tokens = tk_txts[i]\n",
    "        temp = pd.DataFrame()  # creating a temporary dataframe(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "        for w_ind in range(0, len(tokens)): # looping through each word of a single document and spliting through space\n",
    "            try:\n",
    "                word = tokens[w_ind]\n",
    "                word_vec = wv[word] # if word is present in embeddings\n",
    "                temp = temp.append(pd.Series(word_vec), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "            except:\n",
    "                pass\n",
    "        doc_vector = temp.sum() # take the sum of each column\n",
    "        docs_vectors = docs_vectors.append(doc_vector, ignore_index = True) # append each document value to the final dataframe\n",
    "    return docs_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unweighted vector representation for job advertisements descriptions only\n",
    "unweighted_preTW2v = unweighted_vectors(preTW2v,jobs_df['tk_Descriptions'])\n",
    "unweighted_preTW2v.isna().any().sum() # check whether there is any null values in the document vectors dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving outputs\n",
    "Save the count vector representation.\n",
    "- count_vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save output data...\n",
    "def write_vectorFile(job_id,data_features,filename):\n",
    "    num = data_features.shape[0] # Number of job descriptions\n",
    "    out_file = open(filename, 'w') # creates a txt file and open to save the count vectors\n",
    "    for a_ind in range(0, num): # loop through each job description by index\n",
    "        out_file.write('#{}'.format(job_id[a_ind])) # write the job webindex at the beginning of each line\n",
    "        for f_ind in data_features[a_ind].nonzero()[1]: # for each word index that has non-zero entry in the data_feature\n",
    "            value = data_features[a_ind][0,f_ind] # retrieve the value of the entry from data_features\n",
    "            out_file.write(\",{}:{}\".format(f_ind,value)) # write the entry to the file in the format of word_index:value\n",
    "        out_file.write('\\n') # start a new line after each job decription\n",
    "    out_file.close() # close the file\n",
    "\n",
    "write_vectorFile(webindices,count_features,\"count_vectors.txt\") # write the count vector to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Job Advertisement Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Language model comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The count vectors language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy for Fold 1 : 0.8461538461538461\n",
      "Model accuracy for Fold 2 : 0.8269230769230769\n",
      "Model accuracy for Fold 3 : 0.7843137254901961\n",
      "Model accuracy for Fold 4 : 0.8431372549019608\n",
      "Model accuracy for Fold 5 : 0.9019607843137255\n",
      "The average accuracy of the model is: 0.8404977375565611\n"
     ]
    }
   ],
   "source": [
    "# Testing the count vector language model with Description of job advertisements as the feature for the model\n",
    "seed = 0\n",
    "cv = KFold(n_splits=5, random_state=seed, shuffle=True) # setting k-fold to 5 for cross validation\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(count_features, jobs_df['Category'], test_size=0.33, random_state=seed)\n",
    "\n",
    "model = LogisticRegression(max_iter = 2000, random_state=seed) # increase the max_iter to 2000 for convergence\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv) # calculated the accuracy score for each fold\n",
    "for i in range(len(scores)):\n",
    "    print('Model accuracy for Fold',i+1, ':', scores[i])\n",
    "print('The average accuracy of the model is:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The TF-IDF weighted vectors language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy for Fold 1 : 0.9038461538461539\n",
      "Model accuracy for Fold 2 : 0.8461538461538461\n",
      "Model accuracy for Fold 3 : 0.9215686274509803\n",
      "Model accuracy for Fold 4 : 0.9215686274509803\n",
      "Model accuracy for Fold 5 : 0.9019607843137255\n",
      "The average accuracy of the model is: 0.8990196078431373\n"
     ]
    }
   ],
   "source": [
    "# Testing the TF-IDF weighted language model with Description of job advertisements as the feature for the model\n",
    "seed = 0\n",
    "cv = KFold(n_splits=5, random_state=seed, shuffle=True) # setting k-fold to 5 for cross validation\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(weighted_preTW2v, jobs_df['Category'], test_size=0.33, random_state=seed)\n",
    "\n",
    "model = LogisticRegression(max_iter = 2000, random_state=seed) # increase the max_iter to 2000 for convergence\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv) # calculated the accuracy score for each fold\n",
    "for i in range(len(scores)):\n",
    "    print('Model accuracy for Fold',i+1, ':', scores[i])\n",
    "print('The average accuracy of the model is:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The unweighted vectors language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy for Fold 1 : 0.8653846153846154\n",
      "Model accuracy for Fold 2 : 0.8653846153846154\n",
      "Model accuracy for Fold 3 : 0.9019607843137255\n",
      "Model accuracy for Fold 4 : 0.8823529411764706\n",
      "Model accuracy for Fold 5 : 0.8627450980392157\n",
      "The average accuracy of the model is: 0.8755656108597286\n"
     ]
    }
   ],
   "source": [
    "# Testing unweighted language model with Description of job advertisements as the feature for the model\n",
    "seed = 0\n",
    "cv = KFold(n_splits=5, random_state=seed, shuffle=True) # setting k-fold to 5 for cross validation\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(unweighted_preTW2v, jobs_df['Category'], test_size=0.33, random_state=seed)\n",
    "\n",
    "model = LogisticRegression(max_iter = 2000, random_state=seed) # increase the max_iter to 2000 for convergence\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv) # calculated the accuracy score for each fold\n",
    "for i in range(len(scores)):\n",
    "    print('Model accuracy for Fold',i+1, ':', scores[i])\n",
    "print('The average accuracy of the model is:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the three language models and by looking at the accuracy scores in the above output, we can simply say that the TF-IDF weighted vectors language model has the best performance with a close proximity to the unweighted vectors language model. My understanding of this is that if we calculated the weight of the TF-IDF vector representaion then our model will be able to perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Does more information provide higher accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we generate TF-IDF feature representation for the Title\n",
    "title_Vectorizer = TfidfVectorizer(analyzer = \"word\") # initialising the TfidfVectorizer\n",
    "title_features = title_Vectorizer.fit_transform(jobs_df['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy for Fold 1 : 0.7692307692307693\n",
      "Model accuracy for Fold 2 : 0.5769230769230769\n",
      "Model accuracy for Fold 3 : 0.7843137254901961\n",
      "Model accuracy for Fold 4 : 0.7843137254901961\n",
      "Model accuracy for Fold 5 : 0.7058823529411765\n",
      "The average accuracy of the model is: 0.7241327300150829\n"
     ]
    }
   ],
   "source": [
    "# Testing tf-idf language model with Title of job advertisements as the feature for the model\n",
    "seed = 0\n",
    "cv = KFold(n_splits=5, random_state=seed, shuffle=True) # setting k-fold to 5 for cross validation\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(title_features, jobs_df['Category'], test_size=0.33, random_state=seed)\n",
    "\n",
    "model = LogisticRegression(max_iter = 2000, random_state=seed) # increase the max_iter to 2000 for convergence\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv) # calculated the accuracy score for each fold\n",
    "for i in range(len(scores)):\n",
    "    print('Model accuracy for Fold',i+1, ':', scores[i])\n",
    "print('The average accuracy of the model is:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we generate TF-IDF feature representation for the Title and Description\n",
    "jobs_df['concatenated'] = jobs_df['Title'] + jobs_df['Description'] # we concatenated the job title with job description\n",
    "TD_Vectorizer = TfidfVectorizer(analyzer = \"word\") # initialising the TfidfVectorizer\n",
    "TD_features = TD_Vectorizer.fit_transform(jobs_df['concatenated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy for Fold 1 : 0.8269230769230769\n",
      "Model accuracy for Fold 2 : 0.8076923076923077\n",
      "Model accuracy for Fold 3 : 0.803921568627451\n",
      "Model accuracy for Fold 4 : 0.8431372549019608\n",
      "Model accuracy for Fold 5 : 0.9215686274509803\n",
      "The average accuracy of the model is: 0.8406485671191554\n"
     ]
    }
   ],
   "source": [
    "# Testing tf-idf language model with Title and Description of job advertisements as features for the model\n",
    "seed = 0\n",
    "cv = KFold(n_splits=5, random_state=seed, shuffle=True) # setting k-fold to 5 for cross validation\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(TD_features, jobs_df['Category'], test_size=0.33, random_state=seed)\n",
    "\n",
    "model = LogisticRegression(max_iter = 2000, random_state=seed) # increase the max_iter to 2000 for convergence\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv) # calculated the accuracy score for each fold\n",
    "for i in range(len(scores)):\n",
    "    print('Model accuracy for Fold',i+1, ':', scores[i])\n",
    "print('The average accuracy of the model is:', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question \"Does more information provide higher accuracy?\" is more complicated than giving a simple answer of yes or no. But in this case, more information didn't provide higher accuracy for the prediction model. The quality of the information is important in this case, if we cleaned the `Title` of the job description and generate the TF-IDF weighted vectore and then combine it with the weighted TF-IDF vector for the `Desctiptions` then I imagine it would give a higher score than all the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "To summarise, in these two tasks, I produced different feature representations while only considering the job description. Â I then stored the count vector representation of each job description into a file called `count_vectors.txt`. Then, using the GoogleNews300 embedding language model, I built the TF-IDF weighted and unweighted vector representations for each job advertisement description. Then, for Task 3, I developed machine learning models for categorising the text of a job advertisement. I implemented the logistic regression model from `sklearn` for this specific task. Then, I compared the three different language models using two sets of tests on the provided dataset, and I looked into whether having more information might result in a greater performance of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
